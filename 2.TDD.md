- # **技术设计文档 (TDD)**

  ## 1. 概述

  本文件旨在将产品需求文档 (PRD) v3.0 中的"动态记忆森林"概念，转化为具体的技术实现方案。

  **核心架构**：我们将构建一个统一的、由chat_service驱动的**代理增强生成 (AAG) 循环**。在此循环中，我们的AI助手能够利用一个持久化的、由"记忆节点"组成的**摘要树**进行检索和推理。为了实现动态的深度探索，AI助手将拥有调用内部工具的能力，这些工具的能力通过**MCP (Machine Communication Protocol) 协议**进行标准化描述，从而让AI助手能自主决定何时以及如何获取更完整的历史知识。

  ------

  ## 2. 数据模型设计

  为了支持层级化的"记忆树"结构，我们引入一个全新的、以树形结构为核心的数据库模式。

  ### 2.1. 数据库表结构 (Postgresql)

  用于存储记忆树的元数据和层级结构。

  #### `archives` 表 (元数据表)

  ```sql
  -- `archives` 表：代表一个"记忆档案"，即一棵完整的"记忆树"
  CREATE TABLE IF NOT EXISTS archives (
      id SERIAL PRIMARY KEY,
      model_id TEXT NOT NULL,          -- 关联的AI模型的唯一标识符
      name TEXT NOT NULL,              -- 档案的名称 (例如 "Project Phoenix Q3 Report")
      created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
  );
  CREATE INDEX idx_archives_model ON archives (model_id);
  ```

  #### `memory_nodes` 表 (核心树结构表)

  ```sql
  -- `memory_nodes` 表：存储一个记忆档案（树）中的所有节点
  CREATE TABLE IF NOT EXISTS memory_nodes (
      id SERIAL PRIMARY KEY,
      archive_id INTEGER NOT NULL,
      parent_id INTEGER,
      path TEXT,
      depth INTEGER,
      node_type TEXT NOT NULL CHECK(node_type IN ('LEAF_CHUNK', 'SUMMARY_NODE')),
      content TEXT NOT NULL,
      summary TEXT NOT NULL,
      created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
      FOREIGN KEY (archive_id) REFERENCES archives (id) ON DELETE CASCADE,
      FOREIGN KEY (parent_id) REFERENCES memory_nodes(id) ON DELETE SET NULL
  );
  
  -- 为常用查询创建索引
  CREATE INDEX idx_nodes_archive_parent ON memory_nodes (archive_id, parent_id);
  CREATE INDEX idx_nodes_archive_path ON memory_nodes (archive_id, path);
  CREATE INDEX idx_nodes_archive_depth ON memory_nodes (archive_id, depth);
  ```

  **设计理念**:

  -   **`archives` 表**: 作为森林的顶级目录，每个条目指向一棵独立的"记忆树"。
  -   **`memory_nodes` 表**: 是核心，它同时使用两种策略来存储树形结构：
      -   **邻接表 (`parent_id`)**: 最直观的树表示法，用于快速查找直接父/子节点。
      -   **物化路径 (`path`, `depth`)**: 空间换时间的策略，极大加速子树查询（如 `SELECT * FROM memory_nodes WHERE path LIKE '1/5/%'`），避免昂贵的递归SQL查询。路径以'/'结尾，方便使用 `LIKE` 操作符。
  -   **`node_type`**: 明确区分节点性质：原始文本片段（`LEAF_CHUNK`）或摘要（`SUMMARY_NODE`）。
  -   **`content`**: 对于叶子节点(`LEAF_CHUNK`)，`content`是原始文本块；对于摘要节点(`SUMMARY_NODE`)，`content`是其所有直接子节点`content`的拼接。
  -   **`summary`**: 对于`content`的摘要。

  ### 2.2. 向量数据存储 (Qdrant)

  用于存储所有节点摘要的嵌入向量，并实现高性能的相似度搜索。

  -   **Collection**: 我们将创建一个名为 `memory_forest` 的单一集合来存储所有向量。
  -   **Point Structure**: 每个存储在`Qdrant`中的“点”(Point)都与**PostgreSQL**中的一个`memory_node`一一对应。
      
      -   `id`: Point的ID **必须** 与`memory_nodes`表中的`id`完全相同，这是连接两个系统的关键。
      -   `vector`: 存储`summary`的嵌入向量。
      -   `payload`: 存储用于高效过滤的元数据。
          ```json
          {
            "model_id": "claude-3-opus-20240229",
            "archive_id": 12,
            "node_type": "SUMMARY_NODE"
          }
          ```
  -   **数据同步关系**:
      -   **主源**: **PostgreSQL**是节点元数据（`content`, `summary`, `path`等）的真实来源 (Source of Truth)。Qdrant是向量和可搜索元数据（`payload`）的真实来源。
      -   **写操作**: 当创建或更新一个`memory_node`时，必须同时向**PostgreSQL**写入结构化数据，并向`Qdrant`**upsert**对应的向量点。
      -   **读操作 (检索)**: 检索流程总是先查询**Qdrant**获取最相关的`node_id`列表，然后再用这些ID去**PostgreSQL**中查询节点的完整信息。
  
  ## 3. 核心流程：两阶段AAG循环
  
  ### 3.1. 阶段一：智能归档 (构建记忆树)
  
  此过程将原始文档转化为数据库中的一棵或多棵层次化摘要树。整个流程由 `build_memory_tree` 函数编排，在创建节点时，需要**同时**写入PostgreSQL和Qdrant。
  
  ```mermaid
  graph TD
      subgraph "Phase 1: 初始化"
          A[开始: 文档分块<br/>Chunk1, C2, C3, C4, C5] --> B{"创建独立的叶子节点<br/>`_create_leaf_nodes`"};
          B --> N1[Node 1<br/>LEAF_CHUNK];
          B --> N2[Node 2<br/>LEAF_CHUNK];
          B --> N3[Node 3<br/>LEAF_CHUNK];
          B --> N4[Node 4<br/>LEAF_CHUNK];
          B --> N5[Node 5<br/>LEAF_CHUNK];
      end
  
      subgraph "Phase 2: 迭代合并 (循环)"
          C{"1. 识别当前根节点<br/>`_get_current_root_nodes`"};
          C --> D{"2. 查找最佳合并对<br/>(相似度最高且满足条件)<br/>`_find_best_merge_pair`"};
          D --> F{"3. 执行合并<br/>`_merge_nodes`"};
          
          subgraph "合并细节"
              F --> H{"创建父节点 S1<br/>(SUMMARY_NODE)"};
              H --> I{"更新 N2, N3:<br/>- parent_id = S1.id<br/>- 更新 depth & path<br/>`_update_subtree_path`"};
          end
          
          M{"4. 检查循环终止条件<br/>(是否找到可合并的对?)"};
          D -- "找到合并对 (e.g., N2, N3)" --> F;
          D -- "未找到" --> M;
          M -- "Yes" --> C;
          M -- "No" --> P_FINAL;
      end
  
      subgraph "Phase 3: 最终结果"
          P_FINAL[最终形成一棵或多棵树];
      end
  
      A --> C;
  ```
  
  ### 3.2. 阶段二：代理式回忆与推理 (结构化探索)
  
  此过程利用已构建的树进行高效的信息检索。整个流程由 `retrieve_from_memory` 函数编排，使用`EMBEDDING_MODEL`和`RERANKER_MODEL`从向量数据库中检索获取。
  
  #### 步骤 1: 向量检索
  
  -   对用户的问题使用`EMBEDDING_MODEL`进行向量化。
  -   从向量数据库中检索出与我们问题的相似度最高的`Top K`个数据。
  
  #### 步骤 2: 数据归档
  
  -   对这些数据按照`archive_id`划分到不同的集合中，对在同一个集合存在多个数据的情况，使用最近公共祖先替代掉原先的数据。
  
  #### 步骤 3: 档案内MCP探索 (Archive-Specific MCP-Loop) - 结构化深度挖掘
  
  -   AI可以在一个结构化的**摘要树**内进行导航式探索。
      -   **情景**: LLM从高层摘要节点（ID: `405`）中发现信息不足，返回 `[MCP: '需要关于风险缓解策略的细节']`。
      -   **操作**:
          1.  系统不再盲目搜索，而是调用 `_archive_drill_down(node_id=405, ...)`。
          2.  此函数**精确地向下探索一层**，只查找 `parent_id = 405` 的直接子节点。
          3.  对返回的子节点使用`Reranker`模型，计算相关性得分大于阈值的子节点，注入上下文。
  
  ## 4. 函数与模块设计
  
  我们将核心逻辑封装在一个 `MemoryForestManager` 类中，该类依赖于外部的AI模型服务和数据库连接。
  
  ### 4.1. 核心服务与工具 (外部依赖)
  
  -   `LLM_service.get_summary(text: str) -> str`: 调用大模型生成文本摘要。
  -   `Embedding_service.get_embedding(text: str) -> list[float]`: 调用模型生成文本的向量。
  -   `Reranker_service.rerank(query: str, documents: list[dict]) -> list[dict]`: 接收一个查询和一组文档（包含ID和内容），返回根据相关性重排序后的文档列表，并附上相关性得分。
  -   `DB_Connection`: 数据库连接对象，提供执行SQL查询的方法。
  -   `langchain.text_splitter.RecursiveCharacterTextSplitter`: 一个成熟的文本分割库，用于将长文本智能地分割成符合大小限制且保持语义连贯的块。
  -   `utils.find_longest_common_prefix(paths: list[str]) -> str`: 找到一组路径字符串的最长公共前缀。
  
  ### 4.2. 阶段一：记忆树构建函数
  
  #### `build_memory_tree(self, model_id: str, document_name: str, raw_text: str, chunk_size: int, similarity_threshold: float) -> int`
  
  -   **作用**: Orchestration function，将一份原始文档完整地处理成一棵或多棵记忆树。
  -   **参数**:
      -   `model_id`: AI模型标识。
      -   `document_name`: 文档名称，用于创建 `archive`。
      -   `raw_text`: 原始文档全文。
      -   `chunk_size`: 初始分块的最大长度。
      -   `similarity_threshold`: 合并节点的相似度阈值。
  -   **返回**: 创建的 `archive_id`。
  -   **核心逻辑**:
      1.  调用 `_create_archive` 创建档案记录，获取 `archive_id`。
      2.  调用`RecursiveCharacterTextSplitter`将 `raw_text` 分割成 `chunks`。
      3.  调用 `_create_leaf_nodes` 初始化所有叶子节点。
      4.  **进入合并循环**:
          ```python
          while True:
              root_ids = self._get_current_root_node_ids(archive_id)
              if len(root_ids) <= 1:
                  break
              
              pair_to_merge_ids = self._find_best_merge_pair(archive_id, root_ids, similarity_threshold, chunk_size)
              
              if not pair_to_merge_ids:
                  break
              
              # 需要一个辅助函数根据ID获取完整的节点信息 dict
              node_a = self._get_node_details(pair_to_merge_ids[0])
              node_b = self._get_node_details(pair_to_merge_ids[1])
              self._merge_nodes(archive_id, node_a, node_b)
          ```
      5.  返回 `archive_id`。
  
  ---
  
  #### `_create_leaf_nodes(self, archive_id: int, chunks: list[str]) -> None`
  
  -   **作用**: 创建初始的叶子节点。
  -   **逻辑**:
      
      1.  遍历 `chunks` 列表。
      2.  对每个 `chunk`:
          a.  调用 `LLM_service.get_summary(chunk)` 获取摘要 `summary`。
          b.  调用 `Embedding_service.get_embedding(summary)` 获取摘要向量 `embedding`。
          c.  **第一步：写入元数据**。向 `memory_nodes` 表 `INSERT` 一条新记录，并获取新生成的 `node_id`。
          
              ```sql
              INSERT INTO memory_nodes (archive_id, parent_id, depth, node_type, content, summary) VALUES (...) RETURNING id
              ```
          d.  **第二步：写入向量**。向 Qdrant 的 `memory_forest` 集合中 `upsert` 一个新的 Point。
              -   **Point ID**: 使用上一步获取的 `node_id`。
                  -   **Vector**: `embedding`。
                      -   **Payload**: `{ "model_id": ..., "archive_id": ..., "node_type": 'LEAF_CHUNK' }`。
      3.  **二次更新 `path`**: 再次遍历刚刚插入的节点，获取它们的 `id`，然后 `UPDATE` 每个节点的 `path` 字段为 `f"{node_id}/"`。

  #### `_get_current_root_node_ids(self, archive_id: int) -> list[int]`

  -   **作用**: 获取指定档案中所有根节点的ID。
  -   **逻辑**: 执行 `SELECT id FROM memory_nodes WHERE archive_id = ? AND parent_id IS NULL ORDER BY id ASC`，并返回一个ID列表。

  #### `_find_best_merge_pair(self, archive_id: int, root_node_ids: list[int], threshold: float, size_limit: int) -> tuple[int, int] | None`

  -   **作用**: 在一组根节点中，利用向量数据库寻找最适合合并的相邻节点对。
  -   **逻辑**:
      1.  初始化 `best_pair = None`, `max_similarity = -1`。
      2.  遍历 `root_node_ids` 列表中的相邻ID对 `(id_a, id_b)`。
      3.  **调用Qdrant获取向量**: 使用 `client.retrieve(collection_name, ids=[id_a, id_b], with_vectors=True)` 同时获取两个节点的向量。
      4.  **计算相似度**: `similarity = utils.calculate_cosine_similarity(vector_a, vector_b)`。
      5.  **检查合并条件**:
          -   `similarity > threshold`
          -   **获取内容长度**: 从**PostgreSQL**中查询 `id_a` 和 `id_b` 的 `content` 字段，并检查 `len(content_a) + len(content_b) <= size_limit`。
      6.  如果条件满足且 `similarity > max_similarity`，则更新 `max_similarity = similarity` 和 `best_pair = (id_a, id_b)`。
      7.  返回 `best_pair` (一个包含两个节点ID的元组)。

  #### `_merge_nodes(self, archive_id: int, node_a: dict, node_b: dict) -> None`

  -   **作用**: 将两个节点合并，创建一个新的父节点，并更新子节点关系。
  -   **逻辑**:
      1.  **创建新内容**: `new_content = node_a['content'] + "\n---\n" + node_b['content']`。
      2.  **生成新摘要和向量**: `new_summary = LLM_service.get_summary(new_content)`；`new_embedding = Embedding_service.get_embedding(new_summary)`。
      3.  **插入父节点**:
          a.  **写入元数据**: 向 `memory_nodes` 表 `INSERT` 一条新记录 (`SUMMARY_NODE`)。获取新插入的 `parent_id`。
          b.  **写入向量**: 向 Qdrant `upsert` 一个新的 Point，ID为 `parent_id`，向量为 `new_embedding`。
          c.  `UPDATE` 该父节点的 `path`。
      4.  **关联子节点**: `UPDATE memory_nodes SET parent_id = ? WHERE id IN (?, ?)`，将 `node_a` 和 `node_b` 的 `parent_id` 指向新的父节点。
      5.  **更新子树**: 调用 `_update_subtree_paths_and_depth` 来修正被移动的 `node_a` 和 `node_b` 及其所有后代的 `path` 和 `depth`。

  #### `_update_subtree_paths_and_depth(self, parent_id: int, parent_path: str, parent_depth: int)`

  -   **作用**: 递归或迭代地更新一个节点下所有子孙的 `path` 和 `depth`。
  -   **逻辑**: (这是一个关键且复杂的操作，可以用队列实现广度优先遍历)
      1.  创建一个队列，并将 `parent_id` 的直接子节点入队。
      2.  循环直到队列为空：
          a.  出队一个节点 `current_node`。
          b.  获取其父节点的 `path` 和 `depth`。
          c.  计算 `new_depth = parent_depth + 1` 和 `new_path = parent_path + f"{current_node['id']}/"`。
          d.  `UPDATE` `current_node` 的 `depth` 和 `path`。
          e.  查询 `current_node` 的所有子节点，并将它们入队。

  ---

  ### 4.3. 阶段二：记忆检索与探索函数

  这部分函数被重新设计，以支持新的三步检索与推理流程。

  #### `retrieve_from_memory(self, model_id: str, query: str, top_k: int) -> list[dict]`

  -   **作用**: 作为AAG循环的入口点，根据用户查询，在整个记忆森林中进行高效的初步信息检索和归档。
  -   **参数**:
      -   `model_id`: AI模型标识。
      -   `query`: 用户的原始查询。
      -   `top_k`: 初始向量检索返回的最多节点数。
  -   **返回**: 一个"代表性节点"列表。每个节点要么是其所在档案的唯一相关节点，要么是多个相关节点的最近公共祖先(LCA)。
  -   **核心逻辑**:
      1.  **步骤 1: 向量检索**:
          -   `query_embedding = Embedding_service.get_embedding(query)`
          -   `initial_nodes = self._initial_vector_retrieval(model_id, query_embedding, top_k)`
          -   如果 `initial_nodes` 为空, 直接返回空列表。
      2.  **步骤 2: 数据归档与LCA合并**:
          -   创建一个字典 `nodes_by_archive = {}`，按 `archive_id` 对 `initial_nodes`进行分组。
          -   `representative_nodes = []`
          -   遍历 `nodes_by_archive.items()`:
              -   `archive_id, nodes_in_archive = item`
              -   如果 `len(nodes_in_archive) == 1`，将该单个节点直接添加到 `representative_nodes`。
              -   如果 `len(nodes_in_archive) > 1`，调用 `lca_node = self._find_lowest_common_ancestor(archive_id, nodes_in_archive)`，并将返回的 `lca_node` 添加到 `representative_nodes`。
      3.  返回 `representative_nodes` 列表。这个列表将作为上下文提供给LLM进行初步分析。

  #### `_initial_vector_retrieval(self, model_id: str, query_embedding: list[float], top_k: int) -> list[dict]`

  -   **作用**: 在指定`model_id`的所有档案中，执行全局向量搜索，找出与查询最相似的`top_k`个节点。
  -   **逻辑**:
      1.  **使用向量数据库**:
          -   向向量数据库（Qdrant）发出查询，搜索 `payload.model_id == model_id` 且与 `query_embedding` 最相似的 `top_k` 个向量。
          -   获取命中的向量所对应的 `node_id`。
          -   根据 `node_id` 从 `memory_nodes` 表中查询节点的完整信息。
      2.  返回 `top_k` 个节点的字典列表。

  #### `_find_lowest_common_ancestor(self, archive_id: int, nodes: list[dict]) -> dict`

  -   **作用**: (新) 在同一档案内，根据一组节点的`path`属性，找到它们的最近公共祖先(LCA)节点。
  -   **逻辑**:
      1.  从 `nodes` 列表中提取所有节点的 `path` 属性，形成 `paths` 列表。
      2.  调用 `common_prefix = utils.find_longest_common_prefix(paths)`。例如，对于 `["1/5/12/", "1/5/18/"]`，返回 `"1/5/"`。
      3.  如果 `common_prefix` 为空或者等于某个节点的完整 `path`，这意味着其中一个节点是其他所有节点的祖先。在这种情况下，返回该祖先节点。
      4.  从 `common_prefix` 中移除最后一个 `/`，得到LCA节点的`path`。例如，从 `"1/5/"` 得到 `"1/5"`。
      5.  执行SQL查询: `SELECT * FROM memory_nodes WHERE archive_id = ? AND path = ?`，其中 `path` 参数为 `common_prefix`。
      6.  返回查询到的LCA节点。如果未找到（理论上不应发生，除非路径不一致），则可以返回这些节点的共同父节点中深度最浅的一个作为备选方案。

  #### `_archive_drill_down(self, node_id: int, query: str, threshold: float) -> list[dict]`

  -   **作用**: (已更新) 实现MCP指令，从一个指定的摘要节点（`node_id`）向下探索一层，并使用Reranker模型筛选出最相关的子节点。
  -   **逻辑**:
      1.  `SELECT id, summary, content, node_type FROM memory_nodes WHERE parent_id = ?`，查询指定节点的所有直接子节点。
      2.  如果查询结果为空（即 `node_id` 是一个叶子节点），返回一个包含该叶子节点自身信息的列表 `[{'id': node_id, 'content': ..., 'score': 1.0}]`。
      3.  如果存在子节点，准备一个文档列表 `child_documents`，每个元素为 `{'id': child_id, 'text': child_summary}`。
      4.  调用 `reranked_children = Reranker_service.rerank(query, child_documents)`。`reranked_children` 将是一个带有相关性得分的排序列表。
      5.  过滤 `reranked_children`，只保留 `score >= threshold` 的子节点。
      6.  根据过滤后的结果，从数据库中获取这些高相关性子节点的完整信息（特别是 `content`，如果需要的话）。
      7.  返回这些经过筛选和排序的子节点信息列表，准备注入到下一次LLM的上下文中。

  ## 5. Prompt设计

  为了确保与大模型 (LLM) 的交互是可预测和可控的，我们采用结构化的JSON作为Prompt的主要格式。

  ### 5.1. 摘要生成Prompt

  这些Prompt在`build_memory_tree`流程中使用，用于创建记忆树的节点。

  #### 5.1.1. `PROMPT_CREATE_LEAF_SUMMARY` (用于叶子节点)

  在 `_create_leaf_nodes` 函数中，当需要为原始文本块生成初始摘要时使用。

  ```json
  {
    "role": "你是一个严谨的信息归档员。",
    "task": "为下面提供的原始文本块生成一个简洁、客观、信息密集的摘要。这个摘要将作为未来检索的唯一依据，因此必须准确反映文本的核心内容。",
    "input_data": {
      "text": "<RAW_TEXT_CHUNK>"
    },
    "constraints": {
      "max_length_chars": 200,
      "style": "中立、事实性",
      "language": "与输入文本语言保持一致"
    },
    "output_format": {
      "schema": "JSON",
      "json_structure": {
        "summary": "<你生成的摘要文本>"
      }
    }
  }
  ```

  **使用场景**: 在 `_create_leaf_nodes` 中，将每个 `chunk` 填入 `<RAW_TEXT_CHUNK>`。

  ---

  #### 5.1.2. `PROMPT_CREATE_PARENT_SUMMARY` (用于父节点)

  在 `_merge_nodes` 函数中，当需要为合并后的新父节点生成摘要时使用。这个摘要是基于其直接子节点的摘要合成的。

  ```json
  {
    "role": "你是一个高级知识整合专家，擅长从多个相关的信息片段中提炼出更高层次的共同主题。",
    "task": "下面提供了多个子节点的摘要。请将它们融会贯通，生成一个能够概括所有子节点核心内容的、更具抽象性的父级摘要。不要简单地拼接，而是要进行真正的综合与提炼。",
    "input_data": {
      "child_summaries": [
        "<SUMMARY_OF_CHILD_A>",
        "<SUMMARY_OF_CHILD_B>",
        "<SUMMARY_OF_CHILD_C...>"
      ]
    },
    "constraints": {
      "max_length_chars": 200,
      "focus": "寻找共性、上层概念或流程关系",
      "language": "与输入文本语言保持一致"
    },
    "output_format": {
      "schema": "JSON",
      "json_structure": {
        "summary": "<你生成的父级摘要文本>"
      }
    }
  }
  ```

  **使用场景**: 在 `_merge_nodes` 中，将 `node_a['summary']` 和 `node_b['summary']` 填入 `child_summaries` 列表。

  ### 5.2. 代理推理与探索Prompt

  这是AAG循环的核心，在 `retrieve_from_memory` 之后，由外部控制器使用，以决定下一步行动。

  #### 5.2.1. `PROMPT_AGENTIC_REASONING_LOOP`

  此Prompt用于驱动LLM的核心决策：是直接回答问题，还是请求深入探索（发出MCP指令）。

  ```json
  {
    "role": "你是一个名为'记忆森林'的智能研究代理。你的任务是基于一个结构化记忆系统来回答用户的问题。",
    "goal": "根据下方提供的'上下文节点'，判断是否足以直接、完整地回答'用户查询'。如果信息充足，请直接回答；如果信息是高层摘要且细节不足，请发出'EXPLORE'指令，以获取更深层次的信息。",
    "input_data": {
      "user_query": "<USER_QUERY_TEXT>",
      "context_nodes": [
        {
          "node_id": "<NODE_ID_1>",
          "node_type": "<LEAF_CHUNK_OR_SUMMARY_NODE>",
          "summary": "<SUMMARY_OF_NODE_1>",
          "hint": "这是多个相关节点的最近公共祖先(LCA)，可能是一个高层概括。"
        },
        {
          "node_id": "<NODE_ID_2>",
          "node_type": "<LEAF_CHUNK_OR_SUMMARY_NODE>",
          "summary": "<SUMMARY_OF_NODE_2>",
          "hint": "这是一个独立的、高度相关的叶子节点。"
        }
      ]
    },
    "available_actions": [
      {
        "action_name": "ANSWER",
        "description": "当且仅当上下文信息已足够全面回答用户问题时使用此操作。",
        "output_payload": {
          "final_answer": "基于上下文生成的完整回答。"
        }
      },
      {
        "action_name": "EXPLORE",
        "description": "当上下文中的某个节点是'SUMMARY_NODE'类型，且其摘要表明需要更多细节才能回答问题时使用。一次只能探索一个节点。",
        "output_payload": {
          "node_id_to_explore": "<选择一个需要深入的node_id>",
          "reasoning_for_exploration": "<简要说明为什么需要探索这个节点，例如：'需要了解风险缓解策略的具体步骤'。>"
        }
      }
    ],
    "output_format": {
      "schema": "JSON",
      "description": "你的回答必须是以下两种JSON结构之一。",
      "json_structure_options": [
        {
          "action": "ANSWER",
          "payload": {
            "final_answer": "<string>"
          }
        },
        {
          "action": "EXPLORE",
          "payload": {
            "node_id_to_explore": "<integer>",
            "reasoning_for_exploration": "<string>"
          }
        }
      ]
    }
  }
  ```

  **使用场景**:

  1.  `retrieve_from_memory` 返回 `representative_nodes` 列表。
  2.  外部控制器将用户查询和这些节点的信息填入此Prompt。
  3.  系统接收LLM返回的JSON。
  4.  如果 `action` 是 `ANSWER`，流程结束。
  5.  如果 `action` 是 `EXPLORE`，系统解析出 `node_id_to_explore` 和 `reasoning_for_exploration`，然后调用 `_archive_drill_down(node_id=..., query=user_query, ...)`。`reasoning_for_exploration` 可以作为辅助信息传递给Reranker，或者与原始query拼接以获得更好的rerank效果。

  ### 5.3. 关于Reranker模型的说明

  值得注意的是，`Reranker_service` 通常不是一个通过上述复杂Prompt进行交互的生成式LLM。它是一个专门的判别式模型（如Cross-Encoder）。它的API调用非常简单，不涉及JSON格式的Prompt。

  -   **API调用示例**: `reranker.rank(query="风险缓解策略", documents=["摘要1", "摘要2", "摘要3"])`
  -   **返回**: `[{'doc': '摘要2', 'score': 0.98}, {'doc': '摘要1', 'score': 0.75}, ...]`

  因此，我们不需要为Reranker设计对话式的Prompt。TDD中提到的 `Reranker_service.rerank()` 函数直接封装了这种调用。

  ---

  ## 6. MCP (Machine Communication Protocol) 服务器集成

  为了拥抱AI代理生态的行业标准，并使我们的“动态记忆森林”能力能够被外部AI代理（如Gemini, Deepseek等）无缝调用，我们将系统的核心功能通过**MCP协议**作为一套标准化的工具集对外提供。

  ### 6.1. 架构理念：作为工具提供者 (Tool Provider)

  我们的服务不再扮演决策者的角色，而是转变为一个高效、专注的**工具提供者**。外部的AI代理将负责决策逻辑（例如，判断何时需要更多信息），而我们的服务器则响应它们的工具调用请求，提供精准的数据和操作能力。

  **核心交互流程如下：**

  ```mermaid
  sequenceDiagram
      participant User as 用户
      participant Agent as AI代理 (如Cursor)
      participant MemoryForestAPI as 记忆森林API (H.E.R.)
      
      User->>+Agent: "项目Phoenix的风险缓解策略是什么？"
      
      Agent->>Agent: 1. (初步RAG) 可能通过其他方式检索到节点`405`的摘要
      
      Agent->>+MemoryForestAPI: 2. 通过`/mcp`连接，发现可用工具
      MemoryForestAPI-->>-Agent: "你好，我提供一个名为'explore_memory_node'的工具..."
      
      Agent->>Agent: 3. **进行推理决策**<br/>"摘要信息不足，需要调用工具获取细节。"
  
      Agent->>+MemoryForestAPI: 4. **调用工具**<br/>GET /tools/explore_memory_node?node_id=405&query=...
      
      MemoryForestAPI->>MemoryForestAPI: 5. 执行 `memory_service.archive_drill_down()`
      
      MemoryForestAPI-->>-Agent: 6. 返回子节点的JSON列表
      
      Agent->>Agent: 7. 整合新信息，生成最终答案
      
      Agent-->>-User: "关于项目Phoenix的风险缓解策略，具体细节如下：..."
  ```

  ### 6.2. 工具API设计 (`/tools/explore_memory_node`)

  为了让外部AI代理能够与我们的记忆森林交互，我们设计了一个核心的工具API端点。

  -   **端点**: `GET /tools/explore_memory_node`
  -   **职责**: 实现TDD 4.3节中描述的 `_archive_drill_down` 功能。接收一个父节点ID和用户查询，返回其所有相关的直接子节点。
  -   **请求参数**:
      
      -   `node_id` (integer, required): 要探索的父摘要节点的ID。
      -   `query` (string, required): 用户的原始问题，用于`Reranker`模型对子节点进行相关性排序和筛选。
  -   **成功响应 (200 OK)**:
      -   **类型**: `List[MemoryNode]`
      -   **结构**:
          ```json
          [
            {
              "id": 501,
              "summary": "关于市场风险的具体分析和应对措施...",
              "content": "...",
              "node_type": "LEAF_CHUNK",
              "relevance_score": 0.95
            },
            {
              "id": 502,
              "summary": "技术风险的识别与Plan B...",
              "content": "...",
              "node_type": "SUMMARY_NODE",
              "relevance_score": 0.87
            }
          ]
          ```

  ### 6.3. 服务器集成实现

  我们使用 `fastapi-mcp` 库来快速、无缝地将MCP协议支持集成到现有的FastAPI应用中。

  **关键代码 (`src/server.py`)**:

  1.  **导入库**:
      ```python
      from fastapi_mcp import add_mcp_server
      ```
  2.  **实现工具端点**:
      ```python
      @app.get("/tools/explore_memory_node", response_model=List[MemoryNode], tags=["MCP Tools"])
      async def explore_memory_node(...):
          # ... 内部调用 memory_service ...
      ```
  3.  **挂载MCP服务器**: 在主程序启动前，调用`add_mcp_server`将协议挂载到应用上。
      ```python
      add_mcp_server(
          app,
          mount_path="/mcp",
          name="H.E.R. 记忆森林工具",
          tags=["MCP"]
      )
      ```
      `fastapi-mcp`会自动扫描所有FastAPI路径，并将其转换为AI代理可以理解的工具定义，通过`/mcp`端点暴露出去。

  ### 6.4. 对服务层的影响

  -   `memory_service`：需要提供一个公开的、稳定的方法（如`archive_drill_down`），该方法封装了数据库查询、调用Reranker服务等底层逻辑，以供`/tools/explore_memory_node`端点调用。

  - **服务定位**: 整个服务的核心定位是成为一个可通过API调用的、专用的“记忆森林”数据与逻辑处理器，供上层的AI应用或代理进行集成。

  ------

  ## 7. 未来展望：档案合并

  当前的架构有意地将每个主题隔离在独立的档案中。这为未来实现一个高级功能——"**档案合并**"——奠定了基础。

  -   **概念**: 系统可以主动识别出属于同一个 `model_id` 的多个档案（即多棵树）之间存在高度主题关联，并建议用户或自动将它们合并成一棵更全面的、新的记忆树。
  -   **技术实现**: 这可以通过分析不同 `archive` 的根节点 (`depth=0`) 之间的语义相似度来实现。如果多个根节点的相似度很高，可以创建一个新的"超级根节点"，并将这些独立的树作为其子树挂载上去，再调用 `_update_subtree_paths_and_depth` 即可。
  -   **价值**: 这将允许AI像人类一样，将零散的相关知识融会贯通，形成更宏大、更深刻的理解。